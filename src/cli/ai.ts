import { Ollama } from '@langchain/ollama'
import { PromptTemplate } from "@langchain/core/prompts";
import { InMemoryChatMessageHistory } from '@langchain/core/chat_history'
import { input, select } from "@inquirer/prompts"
import { resolve } from 'node:path';
import { parse } from 'yaml'
import { readFile } from 'node:fs/promises';
import chalk from 'chalk'
import { createBanner } from '../util';

const llm = new Ollama({
    model: "qwen2.5",
})

const history = new InMemoryChatMessageHistory()

async function main() {

    console.log(createBanner());
    

    const background = await select({
        message: chalk.hex("f75394").bgGreen.bold("é€‰æ‹©ä½ çš„ ai é¢„è®¾"),
        choices: [
            {
                name: "ðŸ’æœè£…æ•°æ®åˆ†æžå¸ˆ",
                value: "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æœè£…æ•°æ®åˆ†æžå¸ˆï¼Œè¯·ä½ å›žç­”ä»¥ä¸‹å†…å®¹:\n{input}"
            },
            {
                name: "ðŸŒ¸è½¯ä»¶å·¥ç¨‹å¸ˆ",
                value: "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è½¯ä»¶å·¥ç¨‹å¸ˆï¼Œè¯·ä½ å›žç­”ä»¥ä¸‹å†…å®¹:\n{input}"
            },
            {
                name: "ðŸ’®ä»Žé…ç½®ä¸­è¯»å–",
                value: parse(await readFile(resolve(__dirname, "../../config/ai.yaml"), { encoding: "utf-8" })).backgroundSelect
            }
        ]
    })

    const prompt = PromptTemplate.fromTemplate(
        background
    )

    const dataAnalytics = async (input: string) => {
        return await prompt.pipe(llm).invoke({
            input
        })
    }

    while (true) {

        const question = await input({ message: "è¾“å…¥æ‚¨çš„é—®é¢˜", })

        if (question === "exit") {
            console.log("----------",chalk.hex("f75394").bold("[excel-exporter]:"),"å·²é€€å‡º !!!!!!","----------")
            process.kill(process.pid)
        }

        console.log(await dataAnalytics(question));

    }
}


main()