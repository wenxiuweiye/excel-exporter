import { Ollama } from '@langchain/ollama'
import { PromptTemplate } from "@langchain/core/prompts";
import { InMemoryChatMessageHistory } from '@langchain/core/chat_history'
import { input } from "@inquirer/prompts"
import ora from 'ora';

import { createBanner, createFooter, createQuestion, createTitle, createVersion } from '../../util';
import { useChatBg } from './useChatBg';

const llm = new Ollama({
    model: "qwen2.5",
})

export async function aiCommand() {

    const {aiPreset, INPUT, OUTPUT} = await useChatBg()

    const promptTemplate = new PromptTemplate({
        template: "\nè¿™ä¸ªåŽ†å²æ¶ˆæ¯è®°å½•:\n{chatHistory}\n ä»¥ä¸‹æ˜¯ç”¨æˆ·çš„é—®é¢˜:\n{input}",
        inputVariables: ["chatHistory", "input"]
    })


    const history = new InMemoryChatMessageHistory()

    history.addAIMessage(aiPreset)

    const askQuestion = async (input: string) => {

        const chatHistory = await history.getMessages().then(messages => messages.map(message => message.content))

        const formattedPrompt = await promptTemplate.format({
            chatHistory,
            input
        })

        const response = await llm.invoke(formattedPrompt)

        history.addAIMessage(response)

        return response
    }

    while (true) {

        const question = await input({ message: createQuestion("ðŸŒ¸è¾“å…¥æ‚¨çš„é—®é¢˜ (è¾“å…¥exitå³å¯é€€å‡º)"), })

        if (question === "exit") {
            return console.log(createFooter(), createTitle(), "å·²é€€å‡º !!!!!!", createFooter())
        }

        const spinner = ora("æ¨¡åž‹æ­£åœ¨è®¡ç®—å¤„ç†...").start()

        const response = await askQuestion(question)
            .then(message => {
                spinner.stop()
                return message
            })
            .catch(e => {
                spinner.stop()
                throw Error(e)
            })

        console.log(response);
    }
}
